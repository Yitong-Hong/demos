import math
import torch
from torch import nn


class MultiHeadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout=0.1):
        """
        初始化多头注意力机制。

        Args:
            embed_dim (int): 输入和输出的特征维度（必须等于n_dim）。
            num_heads (int): 注意力头的数量。
            dropout (float): Dropout的概率，用于注意力权重后的Dropout。
        """
        super(MultiHeadAttention, self).__init__()
        assert embed_dim % num_heads == 0, "embed_dim must be divisible by num_heads"

        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # 定义线性变换层用于Q, K, V
        self.W_Q = nn.Linear(embed_dim, embed_dim)
        self.W_K = nn.Linear(embed_dim, embed_dim)
        self.W_V = nn.Linear(embed_dim, embed_dim)

        # 最终的线性层
        self.fc = nn.Linear(embed_dim, embed_dim)

        # Dropout层
        self.dropout = nn.Dropout(p=dropout)

        # 层归一化
        self.layer_norm = nn.LayerNorm(embed_dim)

    def forward(self, I):
        """
        前向传播。

        Args:
            I (torch.Tensor): 输入张量，形状为 (batch_size, seq_length, embed_dim)

        Returns:
            torch.Tensor: 输出张量，形状为 (batch_size, seq_length, embed_dim)
        """
        batch_size, seq_length, embed_dim = I.size()

        # 线性变换
        Q = self.W_Q(I)  # (batch_size, seq_length, embed_dim)
        K = self.W_K(I)  # (batch_size, seq_length, embed_dim)
        V = self.W_V(I)  # (batch_size, seq_length, embed_dim)

        # 重塑为多头格式，并转置以适应注意力计算
        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)
        # Q, K, V 形状: (batch_size, num_heads, seq_length, head_dim)

        # 计算注意力得分
        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        # attention_scores 形状: (batch_size, num_heads, seq_length, seq_length)

        # 计算注意力权重
        attention_weights = torch.softmax(attention_scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        # attention_weights 形状: (batch_size, num_heads, seq_length, seq_length)

        # 应用注意力权重到V
        attention_output = torch.matmul(attention_weights, V)
        # attention_output 形状: (batch_size, num_heads, seq_length, head_dim)

        # 合并多头的输出
        attention_output = (attention_output.transpose(1, 2).contiguous().
                            view(batch_size, seq_length, embed_dim))
        # attention_output 形状: (batch_size, seq_length, embed_dim)

        # 通过最终的线性层
        output = self.fc(attention_output)  # (batch_size, seq_length, embed_dim)

        # 添加残差连接并进行层归一化
        output = self.layer_norm(output + I)  # (batch_size, seq_length, embed_dim)

        return output


# 测试代码
if __name__ == "__main__":
    # 定义参数
    batch_size = 2
    seq_length = 4
    embed_dim = 6  # 确保 embed_dim = n_dim
    num_heads = 3
    dropout = 0.1

    # 初始化模型
    model = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout)

    # 创建随机输入
    I = torch.rand(batch_size, seq_length, embed_dim)

    # 打印模型结构
    print("模型结构:")
    print(model)

    # 前向传播
    output = model(I)

    # 打印输出
    print("\n经过多头注意力机制后的输出:")
    print(output)
    print("\n输出形状:", output.shape)
